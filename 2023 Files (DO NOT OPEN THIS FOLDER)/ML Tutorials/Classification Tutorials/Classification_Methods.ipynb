{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd57fa6",
   "metadata": {},
   "source": [
    "# Topics Covered:\n",
    "* Logistic Regression\n",
    "* SVM\n",
    "* KNN\n",
    "* Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf249c",
   "metadata": {},
   "source": [
    "## Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50697f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\", palette=\"Spectral\", style = 'darkgrid' ,font_scale = 1.5, color_codes=True)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e6246",
   "metadata": {},
   "source": [
    "# Classifying the breast cancer data into benign and malignant class<br>\n",
    "\n",
    "1) ID number <br>\n",
    "2) Diagnosis (M = malignant, B = benign)<br>\n",
    "3) radius (mean of distances from center to points on the perimeter)<br>\n",
    "4) texture (standard deviation of gray-scale values)<br>\n",
    "5) perimeter<br>\n",
    "6) area<br>\n",
    "7) smoothness (local variation in radius lengths)<br>\n",
    "8) compactness (perimeter^2 / area - 1.0)<br>\n",
    "9) concavity (severity of concave portions of the contour)<br>\n",
    "10) concave points (number of concave portions of the contour)<br>\n",
    "11) symmetry<br>\n",
    "12) fractal dimension (\"coastline approximation\" - 1)<br>\n",
    "13) to 32) Calculated using mean and worst of the above features***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40fe846c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import some data to play with\n",
    "data = datasets.load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0833b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186cbd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "302d5299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the Data-Set into Training Set and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "634c1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the data\n",
    "#normalized scaler - fit&transform on train, fit only on test\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "n_scaler = MinMaxScaler()\n",
    "X_train_scaled = n_scaler.fit_transform(X_train.astype(np.float))\n",
    "X_test_scaled = n_scaler.transform(X_test.astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b596eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg= LogisticRegression()\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93f329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.967\n",
      "Recall: 0.965\n",
      "Accuracy: 0.965\n",
      "F1 Score: 0.964\n"
     ]
    }
   ],
   "source": [
    "print('Precision: %.3f' % precision_score(y_test, y_pred_logreg, average='weighted'))\n",
    "print('Recall: %.3f' % recall_score(y_test, y_pred_logreg, average='weighted'))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred_logreg))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, y_pred_logreg, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280838be",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "***SVM is a supervised machine learning algorithm capable of performing classification, regression and even outlier detection. The classifier separates data points using a hyperplane with the largest amount of margin. SVM finds an optimal hyperplane which helps in classifying new data points.***\n",
    "\n",
    "C: It is the hyper parameter which control the number of misclassifications errors which has a direct effect on the hyperplane.<br>\n",
    "Gamma: It is used to give weightage to points close to support vector. In other words, changing the value of gamma would change the shape of the hyperplane.<br>\n",
    "Kernal function: If our data is not linearly separable, we could apply a “Kernel Trick” method which maps the nonlinear data to higher dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1217693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.930\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Classification model when data is not normalized\n",
    "from sklearn.svm import SVC\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc_model.predict(X_test)\n",
    "\n",
    "#Measure the accuracy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "print(\"Accuracy score %.3f\" % metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc7ad054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.983\n",
      "Recall: 0.982\n",
      "Accuracy: 0.982\n",
      "F1 Score: 0.982\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Classification model when data is normalized\n",
    "svc_model.fit(X_train_scaled, y_train)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred_scaled = svc_model.predict(X_test_scaled)\n",
    "\n",
    "#Measure the performace\n",
    "print('Precision: %.3f' % precision_score(y_test, y_pred_scaled, average='weighted'))\n",
    "print('Recall: %.3f' % recall_score(y_test, y_pred_scaled, average='weighted'))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred_scaled))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, y_pred_scaled, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f59315",
   "metadata": {},
   "source": [
    "### Effects of SVM parametre \n",
    "**Smaller C**: Lower variance but higher bias (soft margin) and reduce the cost of miss-classification (less penalty).<br>\n",
    "**Larger C**: Lower bias and higher variance (hard margin) and increase the cost of miss-classification (more strict).<br>\n",
    "**Smaller Gamma**: Large variance, far reach, and more generalized solution.<br>\n",
    "**Larger Gamma**: High variance and low bias, close reach, and also closer data points have a higher weight.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109dcdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.983\n",
      "Recall: 0.982\n",
      "Accuracy: 0.982\n",
      "F1 Score: 0.982\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Classification model when data is scaled and kernal function is linear\n",
    "svc_model = SVC(C=1.0,kernel='linear')\n",
    "svc_model.fit(X_train_scaled, y_train)\n",
    "y_pred_new_C=svc_model.predict(X_test_scaled)\n",
    "\n",
    "#Measure the performace\n",
    "print('Precision: %.3f' % precision_score(y_test, y_pred_new_C, average='weighted'))\n",
    "print('Recall: %.3f' % recall_score(y_test, y_pred_new_C, average='weighted'))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred_new_C))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, y_pred_new_C, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e6e67",
   "metadata": {},
   "source": [
    "## KNN\n",
    "KNN classifier creates an imaginary boundary to classify the data. When new data points come in, the algorithm will try to predict that to the nearest of the boundary line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f459146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.983\n",
      "Recall: 0.982\n",
      "Accuracy: 0.982\n",
      "F1 Score: 0.982\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "#Measure the accuracy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "#Measure the performace\n",
    "print('Precision: %.3f' % precision_score(y_test, y_pred_knn, average='weighted'))\n",
    "print('Recall: %.3f' % recall_score(y_test, y_pred_knn, average='weighted'))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred_knn))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, y_pred_knn, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183bb2a9",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "Naive Bayes is a classification algorithm for binary (two-class) and multiclass classification problems. It works based on the Bayes theorem. The easiest naive Bayes classifier to understand is Gaussian Naive Bayes. In this classifier, the assumption is that data from each label is drawn from a simple Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "155642e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.959\n",
      "Recall: 0.956\n",
      "Accuracy: 0.956\n",
      "F1 Score: 0.955\n"
     ]
    }
   ],
   "source": [
    "# Importing the model:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initiating the model without scaling the data:\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "#Measure the performace\n",
    "print('Precision: %.3f' % precision_score(y_test, y_pred, average='weighted'))\n",
    "print('Recall: %.3f' % recall_score(y_test, y_pred, average='weighted'))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0da9de18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.959\n",
      "Recall: 0.956\n",
      "Accuracy: 1.000\n",
      "F1 Score: 0.955\n"
     ]
    }
   ],
   "source": [
    "# Scaling the data\n",
    "nb.fit(X_train_scaled, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred_scaled = nb.predict(X_test_scaled)\n",
    "\n",
    "#Measure the performace\n",
    "print('Precision: %.3f' % precision_score(y_test, y_pred_scaled, average='weighted'))\n",
    "print('Recall: %.3f' % recall_score(y_test, y_pred_scaled, average='weighted'))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_pred, y_pred_scaled))\n",
    "print('F1 Score: %.3f' % f1_score(y_test, y_pred_scaled, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6842e5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
